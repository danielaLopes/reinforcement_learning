{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Cliff problem\n",
    "Consider the following problem. An agent must navigate the grid world represented in Fig. 1, where the grey area corresponds to a cliff that the agent must avoid. The goal state corresponds to the cell marked with a G, while the cell marked with an S corresponds to a starting state. At every step, the agent receives a reward of -1 except in the cliff region, where the reward is -100. Whenever the agent steps into a cliff state, its position is reset back to the start state. When the agent steps into the goal state, the episode ends. The agent has available four actions: up (U), down (D), left (L) and right (R), all of which move the agent deterministically in the corresponding direction.\n",
    "\n",
    "In this question, you will compare the performance of SARSA and Q-learning on the cliff task. To do so, assume that the agent follows an epsilon-greedy policy, with epsilon = 0.15. Run both algorithms for 500 episodes, making sure that the Q-values for both methods are initialized to 0. Consider throughout that gamma = 1 and use a step-size of alpha = 0.5.\n",
    "\n",
    "## Question 1.\n",
    "Compare:\n",
    "* The total reward in each episode for Q-learning and SARSA, plotting the two in a single plot.\n",
    "* The resulting policy after the 500 episodes.\n",
    "Comment any differences observed.\n",
    "**Note:** To mitigate the effect of noise on the plot, perform multiple runs and average the result across runs.\n",
    "\n",
    "In Question 1 you implemented Q-learning and SARSA in a simple grid world domain, where exact representations for q* were possible. However, many domains are not amenable to such exact representations, due to the large size of the corresponding state spaces. In such domains, some form of function approximation is necessary. In the remainder of the homework, you will look at the problem of function approximation in RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS = 50 # Mitigate the effect of noise by averaging over x runs\n",
    "EPISODES = 500\n",
    "#EPSILON = 0.15\n",
    "EPSILON = 0.1\n",
    "GAMMA = 1\n",
    "ALPHA = 0.5\n",
    "REWARD = -1\n",
    "REWARD_CLIFF = -100\n",
    "\n",
    "WIDTH = 12\n",
    "HEIGHT = 4\n",
    "\n",
    "ACTIONS = ('up', 'down', 'left', 'right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    x: int\n",
    "    y: int\n",
    "    action_idx: int\n",
    "    q_value: int\n",
    "\n",
    "    def __init__(self, x: int, y: int, action_idx: int, q_value: int):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.action_idx = action_idx\n",
    "        self.q_value = q_value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'({self.x}, {self.y}, {ACTIONS[self.action_idx]}, {self.q_value})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    width: int\n",
    "    height: int\n",
    "    done: bool # whether the agent has arrived the goal G\n",
    "    states: list[State] # required to update the rewards at the end of each episode\n",
    "    \n",
    "    def __init__(self, width, height):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.x = 0\n",
    "        self.y = self.height - 1\n",
    "        self.done = False\n",
    "        self.states = []\n",
    "        return self.x, self.y\n",
    "    \n",
    "    def _is_cliff(self, x: int, y: int) -> bool:\n",
    "        #return y == self.height - 1 and x != self.width - 1 and x != 0\n",
    "        return y == self.height - 1 and 0 < x < self.width - 1\n",
    "    \n",
    "    def _is_goal(self, x: int, y: int) -> bool:\n",
    "        #return y == self.height - 1 and x == self.width - 1\n",
    "        return y == self.height - 1 and x == self.width - 1\n",
    "\n",
    "    def step(self, action, action_idx) -> int:\n",
    "        if action == 'down':\n",
    "            self.y = min(self.y + 1, self.height - 1)\n",
    "        elif action == 'right':\n",
    "            self.x = min(self.x + 1, self.width - 1)\n",
    "        elif action == 'up':\n",
    "            self.y = max(self.y - 1, 0)\n",
    "        elif action == 'left':\n",
    "            self.x = max(self.x - 1, 0)\n",
    "        else:\n",
    "            raise ValueError('Invalid action')\n",
    "        \n",
    "        reward = 0\n",
    "\n",
    "        if self._is_cliff(self.x, self.y):\n",
    "            print(\"Cliff!\")\n",
    "            #self.reset()\n",
    "            self.done = True\n",
    "            reward = REWARD_CLIFF\n",
    "        elif self._is_goal(self.x, self.y):\n",
    "            print(\"Goal!\")\n",
    "            self.done = True\n",
    "            reward = 0\n",
    "        else:\n",
    "            reward = REWARD\n",
    "        \n",
    "        state = State(self.x, self.y, action_idx, reward)\n",
    "        self.states.append(state)\n",
    "        \n",
    "        return state \n",
    "        \n",
    "    def render(self):\n",
    "        for i in range(self.height):\n",
    "            for j in range(self.width):\n",
    "                # agent\n",
    "                if i == self.y and j == self.x:\n",
    "                    print('A', end='')\n",
    "                # goal\n",
    "                elif self._is_goal(j, i):\n",
    "                    print('G', end='')\n",
    "                # cliff \n",
    "                elif self._is_cliff(j, i):\n",
    "                    print('C', end='')\n",
    "                else:\n",
    "                    print('X', end='')\n",
    "            print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(ABC):\n",
    "    epsilon: float\n",
    "    gamma: float\n",
    "    alpha: float\n",
    "    actions: list[str]\n",
    "    env: Environment\n",
    "    q_table: np.ndarray\n",
    "\n",
    "    def __init__(self, epsilon, gamma, alpha, actions: list[str], env: Environment):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.actions = actions\n",
    "        self.env = env\n",
    "        self.q_table = np.zeros((self.env.width, self.env.height, len(self.actions)))\n",
    "\n",
    "    def reset(self):\n",
    "        self.q_table = np.zeros((self.env.width, self.env.height, len(self.actions)))\n",
    "\n",
    "    def _choose_action(self) -> (str, int):\n",
    "        \"\"\"\n",
    "        Choose an action based on epsilon-greedy policy.\n",
    "        \"\"\"\n",
    "        action_idx = 0\n",
    "        # Explore\n",
    "        if np.random.uniform(0, 1) <= self.epsilon:\n",
    "            action_idx = np.random.choice(range(len(self.actions)))\n",
    "        # Exploit\n",
    "        else:\n",
    "            action_idx = np.argmax(self.q_table[self.env.x, self.env.y])\n",
    "        action = self.actions[action_idx]\n",
    "        return action, action_idx\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _update_rewards(self, \n",
    "                        current_x: int, \n",
    "                        current_y: int, \n",
    "                        current_action_idx: int, \n",
    "                        reward, next_x: int, \n",
    "                        next_y: int, \n",
    "                        next_action_idx: int) -> (int, list[str]):\n",
    "        \"\"\"\n",
    "        Update the rewards for the current state-action pair.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def play_episode(self) -> int:\n",
    "        current_x, current_y = self.env.x, self.env.y\n",
    "        current_action, current_action_idx = self._choose_action()\n",
    "        total_reward = 0\n",
    "        while self.env.done == False:\n",
    "            next_state = self.env.step(current_action, current_action_idx)\n",
    "            print(f\"State: {next_state}\")\n",
    "            next_x, next_y = next_state.x, next_state.y\n",
    "            reward = next_state.q_value\n",
    "            total_reward += reward\n",
    "            next_action, next_action_idx = self._choose_action()\n",
    "            self._update_rewards(current_x, current_y, current_action_idx, reward, next_x, next_y, next_action_idx)\n",
    "            current_x, current_y, current_action, current_action_idx = next_x, next_y, next_action, next_action_idx\n",
    "            #self.env.render()\n",
    "        #self.env.reset()\n",
    "        return total_reward\n",
    "    \n",
    "    def print_policy(self) -> int:\n",
    "        print(\"---> Agent's policy:\")\n",
    "        current_action_idx = np.argmax(self.q_table[self.env.x, self.env.y])\n",
    "        current_action = self.actions[current_action_idx]\n",
    "        total_reward = 0\n",
    "        while self.env.done == False:\n",
    "            next_state = self.env.step(current_action, current_action_idx)\n",
    "            print(f\"State: {next_state}\")\n",
    "            reward = next_state.q_value\n",
    "            total_reward += reward\n",
    "            next_action_idx = np.argmax(self.q_table[self.env.x, self.env.y])\n",
    "            next_action = self.actions[next_action_idx]\n",
    "            current_action, current_action_idx = next_action, next_action_idx\n",
    "        return total_reward, self.env.states\n",
    "\n",
    "class SarsaAgent(Agent):\n",
    "    def _update_rewards(self, current_x: int, \n",
    "                        current_y: int, \n",
    "                        current_action_idx: int, \n",
    "                        reward, next_x: int, \n",
    "                        next_y: int, \n",
    "                        next_action_idx: int):\n",
    "        current_q = self.q_table[current_x, current_y, current_action_idx]\n",
    "        next_q = self.q_table[next_x, next_y, next_action_idx]\n",
    "        target = reward + self.gamma * next_q\n",
    "        self.q_table[current_x, current_y, current_action_idx] += self.alpha * (target - current_q)\n",
    "\n",
    "class QLearningAgent(Agent):\n",
    "    def _update_rewards(self, \n",
    "                        current_x: int, \n",
    "                        current_y: int, \n",
    "                        current_action_idx: int, \n",
    "                        reward, next_x: int, \n",
    "                        next_y: int, \n",
    "                        next_action_idx: int):\n",
    "        current_q = self.q_table[current_x, current_y, current_action_idx]\n",
    "        next_max_q = np.max(self.q_table[next_x, next_y])\n",
    "        target = reward + self.gamma * next_max_q\n",
    "        self.q_table[current_x, current_y, current_action_idx] += self.alpha * (target - current_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    episodes: int\n",
    "    agent: Agent\n",
    "\n",
    "    def __init__(self, episodes: int, agent: Agent) -> None:\n",
    "        self.episodes = episodes\n",
    "        self.agent = agent\n",
    "\n",
    "    def run(self) -> list[int]:\n",
    "        total_rewards = []\n",
    "        for i in range(self.episodes):\n",
    "            print(f\"\\n---------- EPISODE {i} ----------\")\n",
    "            total_reward = self.agent.play_episode()\n",
    "            #self.agent.reset()\n",
    "            self.agent.env.reset()\n",
    "            total_rewards.append(total_reward)\n",
    "        # prints set of actions performed in the last episode\n",
    "        return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards_sarsa = np.zeros((RUNS, EPISODES))\n",
    "for run in tqdm.tqdm(range(RUNS)):    \n",
    "    sarsa_env = Environment(WIDTH, HEIGHT)\n",
    "    #sarsa_env.render()\n",
    "    sarsa_agent = SarsaAgent(EPSILON, GAMMA, ALPHA, ACTIONS, sarsa_env)\n",
    "    sarsa_game = Game(EPISODES, sarsa_agent)\n",
    "    total_rewards_sarsa[run] = sarsa_game.run()\n",
    "    if run == RUNS-1:\n",
    "        sarsa_agent.print_policy()\n",
    "#print(f\"total_rewards_sarsa: {total_rewards_sarsa}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rewards_q_learning = np.zeros((RUNS, EPISODES))\n",
    "for run in range(RUNS):     \n",
    "    qlearning_env = Environment(WIDTH, HEIGHT)\n",
    "    #qlearning_env.render()\n",
    "    qlearning_agent = QLearningAgent(EPSILON, GAMMA, ALPHA, ACTIONS, qlearning_env)\n",
    "    qlearning_game = Game(EPISODES, qlearning_agent)\n",
    "    total_rewards_q_learning[run] = qlearning_game.run()\n",
    "    if run == RUNS-1:\n",
    "        qlearning_agent.print_policy()\n",
    "#print(f\"total_rewards_q_learning: {total_rewards_q_learning}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rewards_sum_per_episode_comp(sarsa_rewards, qlearning_rewards):\n",
    "    plt.plot(range(EPISODES), sarsa_rewards, label='Sarsa')\n",
    "    plt.plot(range(EPISODES), qlearning_rewards, label='Q-Learning')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Sum of rewards during episode')\n",
    "    plt.ylim([-150, 0])\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rewards_sarsa = np.mean(total_rewards_sarsa, axis=0)\n",
    "avg_rewards_q_learning = np.mean(total_rewards_q_learning, axis=0)\n",
    "plot_rewards_sum_per_episode_comp(avg_rewards_sarsa, avg_rewards_q_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final policies obtained were:\n",
    "* SARSA: \n",
    "    State: (0, 2, up, -1)\n",
    "    State: (0, 1, up, -1)\n",
    "    State: (1, 1, right, -1)\n",
    "    State: (1, 0, up, -1)\n",
    "    State: (2, 0, right, -1)\n",
    "    State: (3, 0, right, -1)\n",
    "    State: (4, 0, right, -1)\n",
    "    State: (5, 0, right, -1)\n",
    "    State: (6, 0, right, -1)\n",
    "    State: (7, 0, right, -1)\n",
    "    State: (8, 0, right, -1)\n",
    "    State: (9, 0, right, -1)\n",
    "    State: (10, 0, right, -1)\n",
    "    State: (11, 0, right, -1)\n",
    "    State: (11, 1, down, -1)\n",
    "    State: (11, 2, down, -1)\n",
    "    Goal!\n",
    "    State: (11, 3, down, 0)\n",
    "* Q-learning:\n",
    "    State: (0, 2, up, -1)\n",
    "    State: (1, 2, right, -1)\n",
    "    State: (2, 2, right, -1)\n",
    "    State: (3, 2, right, -1)\n",
    "    State: (4, 2, right, -1)\n",
    "    State: (5, 2, right, -1)\n",
    "    State: (6, 2, right, -1)\n",
    "    State: (7, 2, right, -1)\n",
    "    State: (8, 2, right, -1)\n",
    "    State: (9, 2, right, -1)\n",
    "    State: (10, 2, right, -1)\n",
    "    State: (11, 2, right, -1)\n",
    "    Goal!\n",
    "    State: (11, 3, down, 0)\n",
    "\n",
    "To obtain this plot, I averaged the sum of rewards across 50 different runs. In the first episodes, both SARSA and Q-learning are making several suboptimal decisions, obtaining very negative sums of rewards between -110 and -140. As the number of episodes increases, the sums of rewards for both algorithms increase very fast, stabilizing around after 100 episodes. After these 100 episodes, we can observe the trend that most distinguishes both algorithms for this scenario: SARSA receives slightly higher sums of rewards and has less variability than Q-learning. This is due to SARSA, despite not achieving the optimal policy, being an on-policy algorithm, it uses the next action it takes to update its q-values. This way, it learns that going close to the edge of the cliff can be costly (even after updating the q-values for several episodes, it still performs exploratory actions due to the epsilon greedy policy) and stays away from the cliff. Q-learning, on the other hand, is an off-policy method, meaning that it does not use the policy used to choose the next action to update the q-values. It uses a slightly different policy, in which it chooses the action with highest q-value and does not take into account the epsilon-exploration, so it does not think that going to the edge is costly. This allows it to learn the optimal policy, but it ends up performing worse in terms of sums of rewards because the agent will end up falling in the cliff more easily, since all it takes is an exploratory action to fall into the cliff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
