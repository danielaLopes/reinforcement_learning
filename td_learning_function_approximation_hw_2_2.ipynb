{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD learning with function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_STATES = 7\n",
    "MAX_K = 15\n",
    "GAMMA = 0.99\n",
    "ACTIONS = ['A', 'B']\n",
    "REWARD = 0\n",
    "# Policy: agent selects action A with probability PROB_A \n",
    "# and action B with probability PROB_B\n",
    "PROB_A = 1/7\n",
    "PROB_B = 6/7\n",
    "ALPHA = 0.01 # step size\n",
    "TIME_STEPS = 500\n",
    "N_RUNS = 100 # Mitigate the effect of noise by averaging over x runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDLearning(ABC):\n",
    "    time_steps: int\n",
    "    n_states: int\n",
    "    reward: int\n",
    "    actions: list[str]\n",
    "    # A and B are the possible actions\n",
    "    # transition probability matrices for actions A and B\n",
    "    p_A: np.ndarray\n",
    "    p_B: np.ndarray\n",
    "    # feature matrices for every state in actions A and B\n",
    "    # Each matrix represents a feature function for the K-th feature. \n",
    "    # Each feature vector that composes the matrix represents the \n",
    "    # feature vector for a pair (s, a).\n",
    "    phi_A: np.ndarray\n",
    "    phi_B: np.ndarray\n",
    "    # policy\n",
    "    prob_A: float\n",
    "    prob: float\n",
    "    # parameter vector\n",
    "    w: np.ndarray\n",
    "    # remaining parameters\n",
    "    alpha: float\n",
    "    gamma: float\n",
    "    max_k: int\n",
    "\n",
    "    def __init__(self, time_steps, n_states, reward, actions, prob_A, prob_B, alpha, gamma, max_k) -> None:\n",
    "        self.time_steps = time_steps\n",
    "        self.n_states = n_states\n",
    "        self.reward = reward\n",
    "        self.actions = actions\n",
    "        self.p_A = np.zeros((n_states, n_states))\n",
    "        # Set last column to 1\n",
    "        self.p_A[:, -1] = 1\n",
    "        self.p_B = np.zeros((n_states, n_states))\n",
    "        # Set all columns but last to 1/6\n",
    "        self.p_B[:, :-1] = 1/6\n",
    "        self.phi_A = np.array([[2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0]])\n",
    "        self.phi_B = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "        self.prob_A = prob_A\n",
    "        self.prob_B = prob_B\n",
    "        self.w = np.array([1, 1, 1, 1, 1, 1, 10, 1, 1, 1, 1, 1, 1, 1, 1])\n",
    "        self.alpha = alpha  \n",
    "        self.gamma = gamma\n",
    "        self.max_k = max_k\n",
    "\n",
    "    def _get_parameter_vector_norm(self):\n",
    "        return np.linalg.norm(self.w)\n",
    "\n",
    "    def _get_feature_vector(self, state, action):\n",
    "        if action == 'A':\n",
    "            return self.phi_A[state]\n",
    "        elif action == 'B':\n",
    "            return self.phi_B[state]\n",
    "        \n",
    "    def _q_function(self, state, action):\n",
    "        return np.matmul(self._get_feature_vector(state, action), self.w)\n",
    "\n",
    "    def _choose_action(self) -> str:\n",
    "        return np.random.choice(self.actions, p=[self.prob_A, self.prob_B])\n",
    "\n",
    "    @abstractmethod\n",
    "    def _update_w(self):\n",
    "        pass\n",
    "\n",
    "    def game(self) -> tuple[np.ndarray, list[float]]:\n",
    "        time_steps = np.arange(self.time_steps)\n",
    "        norms = []\n",
    "        cur_state = np.random.choice(range(self.n_states))\n",
    "        cur_action = self._choose_action()\n",
    "        for time_step in range(self.time_steps):\n",
    "            print(f\"\\n------ TIME STEP {time_step} ------\")\n",
    "            if cur_action == 'A':\n",
    "                next_state = np.random.choice(range(self.n_states), p=self.p_A[cur_state])\n",
    "            elif cur_action == 'B':\n",
    "                next_state = np.random.choice(range(self.n_states), p=self.p_B[cur_state])\n",
    "            next_action = self._choose_action()\n",
    "            reward = self.reward\n",
    "            self._update_w(cur_state, cur_action, next_state, next_action, reward)\n",
    "            cur_state = next_state\n",
    "            norms.append(self._get_parameter_vector_norm())\n",
    "        return time_steps, norms\n",
    "\n",
    "class Sarsa(TDLearning):\n",
    "    def _update_w(self, cur_state, cur_action, next_state, next_action, reward):\n",
    "        part1 = self.alpha * self._get_feature_vector(cur_state, cur_action) \n",
    "        part2 = reward + self.gamma * self._q_function(next_state, next_action) - self._q_function(cur_state, cur_action)\n",
    "        self.w = self.w + part1 * part2\n",
    "\n",
    "class QLearning(TDLearning):\n",
    "    def _update_w(self, cur_state, cur_action, next_state, next_action, reward):\n",
    "        part1 = self.alpha * self._get_feature_vector(cur_state, cur_action)\n",
    "        part2 = reward + self.gamma * np.max([self._q_function(next_state, 'A'), self._q_function(next_state, 'B')]) - self._q_function(cur_state, cur_action)\n",
    "        self.w = self.w + part1 * part2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_multiple_runs_norms = np.zeros((N_RUNS, TIME_STEPS))\n",
    "qlearning_multiple_runs_norms = np.zeros((N_RUNS, TIME_STEPS))\n",
    "for run in range(N_RUNS):   \n",
    "    print(f\"\\n\\n===== RUN {run} =====\")   \n",
    "    sarsa = Sarsa(TIME_STEPS, N_STATES, REWARD, ACTIONS, PROB_A, PROB_B, ALPHA, GAMMA, MAX_K)\n",
    "    qlearning = QLearning(TIME_STEPS, N_STATES, REWARD, ACTIONS, PROB_A, PROB_B, ALPHA, GAMMA, MAX_K)\n",
    "    sarsa_time_steps, sarsa_multiple_runs_norms[run] = sarsa.game()\n",
    "    qlearning_time_steps, qlearning_multiple_runs_norms[run] = qlearning.game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_norms(time_steps, sarsa_avg_norms, qlearning_avg_norms):\n",
    "    plt.plot(time_steps, sarsa_avg_norms, label='Sarsa')\n",
    "    plt.plot(time_steps, qlearning_avg_norms, label='Q-learning')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Time steps')\n",
    "    plt.ylabel('Parameter vector norm ||w||')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_avg_norms = np.mean(sarsa_multiple_runs_norms, axis=0)\n",
    "qlearning_avg_norms = np.mean(qlearning_multiple_runs_norms, axis=0)\n",
    "plot_norms(sarsa_time_steps, sarsa_avg_norms, qlearning_avg_norms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SARSA: The norm of the parameter vector for SARSA seems to decrease initially and then stabilizes relatively quickly. This could indicate that the SARSA algorithm is converging to a solution where the updates to the weights become smaller over time, suggesting that the algorithm is stabilizing its policy.\n",
    "\n",
    "* Q-learning: In contrast, the parameter vector norm for Q-learning is consistently increasing over time. This trend might suggest that the weights are growing without stabilization, which could be a sign of divergence or that the algorithm is still actively learning and adjusting its policy. Most likely it did not converge.\n",
    "\n",
    "* Stability: SARSA seems to be more stable in this scenario compared to Q-learning. This could be due to SARSA being an on-policy algorithm, meaning it learns the value of the policy it follows, possibly leading to more conservative updates.\n",
    "\n",
    "* Exploration vs. Exploitation: The increasing trend in the Q-learning norm could be a result of its off-policy nature, where it learns the value of the best possible policy while following another policy. This might cause larger updates if the environment has many states and actions to explore.\n",
    "\n",
    "* Potential Overfitting or Overshooting: The increasing norm in Q-learning might also indicate potential overfitting or overshooting of the value function approximation. It's possible that Q-learning's greedy nature in updating its policy could be leading to over-estimations of the action values.\n",
    "\n",
    "* Learning Speed: The plot does not necessarily reflect the speed of learning in terms of how quickly each algorithm finds a good policy, but rather the magnitude of the updates they make to their parameter vectors.\n",
    "\n",
    "* Algorithm Suitability: Depending on the specific environment and the reward structure, one algorithm may be more suitable than the other. This plot might suggest that SARSA is more appropriate for this particular MDP if stability is preferred.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
